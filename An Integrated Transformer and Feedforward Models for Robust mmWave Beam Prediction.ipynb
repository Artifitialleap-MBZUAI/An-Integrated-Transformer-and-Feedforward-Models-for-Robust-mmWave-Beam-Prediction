{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61701eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n",
      "Training Base Station 1, Epoch 1\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 2\n",
      "Progress: [44%]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19958/1412494545.py:16: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  In_set = In_set_file['DL_input'].astype(np.float32)  # Convert to float32 if complex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 3\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 4\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 5\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 6\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 7\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 8\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 9\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 10\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 11\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 12\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 13\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 14\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 15\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 16\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 17\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 18\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 19\n",
      "Progress: [100%]\n",
      "Training Base Station 1, Epoch 20\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 1\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 2\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 3\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 4\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 5\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 6\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 7\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 8\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 9\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 10\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 11\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 12\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 13\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 14\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 15\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 16\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 17\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 18\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 19\n",
      "Progress: [100%]\n",
      "Training Base Station 2, Epoch 20\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 1\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 2\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 3\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 4\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 5\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 6\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 7\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 8\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 9\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 10\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 11\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 12\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 13\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 14\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 15\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 16\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 17\n",
      "Progress: [100%]\n",
      "Training Base Station 3, Epoch 18\n",
      "Progress: [16%]Attack\n",
      "Base Station 1 Adversarial Test Loss: 0.0012200115098884062\n",
      "Base Station 2 Adversarial Test Loss: 0.0011357913026586175\n",
      "Base Station 3 Adversarial Test Loss: 0.008793005054550511\n",
      "Base Station 4 Adversarial Test Loss: 0.008003673848829098\n",
      "Mean Adversarial MSE across all Base Stations: 0.004788120428981658\n",
      "Adversarial Training\n",
      "Adversarial Training Epoch 1\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 2\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 3\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 4\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 5\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 6\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 7\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 8\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 9\n",
      "Progress: [100%]\n",
      "Adversarial Training Epoch 10\n",
      "Progress: [100%]\n",
      "Base Station 1 Combined Test Loss: 0.0007205470797738858\n",
      "Base Station 2 Combined Test Loss: 0.0006258177288275744\n",
      "Base Station 3 Combined Test Loss: 0.0007658520563771683\n",
      "Base Station 4 Combined Test Loss: 0.0008075475010887853\n",
      "Mean Combined Test MSE across all Base Stations: 0.0007299410915168534\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertConfig\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Loading data\n",
    "In_set_file = loadmat('DLCB_dataset/O1_60/DLCB_input.mat')\n",
    "Out_set_file = loadmat('DLCB_dataset/O1_60/DLCB_output.mat')\n",
    "\n",
    "In_set = In_set_file['DL_input'].astype(np.float32)  # Convert to float32 if complex\n",
    "Out_set = Out_set_file['DL_output'].astype(np.float32)  # Convert to float32 if complex\n",
    "\n",
    "num_user_tot = In_set.shape[0]\n",
    "DL_size_ratio = 0.2\n",
    "DL_size = int(num_user_tot * DL_size_ratio)\n",
    "\n",
    "np.random.seed(2016)\n",
    "num_train = int(DL_size * 0.8)\n",
    "num_test = int(num_user_tot * 0.2)\n",
    "\n",
    "train_index, test_index = train_test_split(range(num_user_tot), test_size=num_test, random_state=2016)\n",
    "In_train, In_test = In_set[train_index], In_set[test_index]\n",
    "Out_train, Out_test = Out_set[train_index], Out_set[test_index]\n",
    "\n",
    "# Number of base stations\n",
    "num_base_stations = 4\n",
    "training_history = [[] for _ in range(num_base_stations)]\n",
    "# Divide input and output data into segments for each base station\n",
    "input_segments_train = torch.chunk(torch.tensor(In_train), num_base_stations, dim=1)\n",
    "output_segments_train = torch.chunk(torch.tensor(Out_train), num_base_stations, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the HybridBeamformingModel\n",
    "class HybridBeamformingModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(HybridBeamformingModel, self).__init__()\n",
    "\n",
    "        # Attention-based model\n",
    "        self.attention_model = AttentionModel(input_size, output_size)\n",
    "\n",
    "        # Feedforward model\n",
    "        self.feedforward_model = FeedforwardModel(input_size, output_size)\n",
    "\n",
    "        # Additional layers for combining outputs\n",
    "        self.merge_fc1 = nn.Linear(output_size * 2, 128)\n",
    "        self.merge_relu = nn.ReLU()\n",
    "        self.merge_fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Forward pass through attention-based model\n",
    "        attention_output = self.attention_model(input_data)\n",
    "\n",
    "        # Forward pass through feedforward model\n",
    "        feedforward_output = self.feedforward_model(input_data)\n",
    "\n",
    "        # Concatenate the outputs from both models\n",
    "        combined_output = torch.cat([attention_output, feedforward_output], dim=-1)\n",
    "\n",
    "        # Pass through additional layers for combining outputs\n",
    "        merged_output = self.merge_fc1(combined_output)\n",
    "        merged_output = self.merge_relu(merged_output)\n",
    "        final_output = self.merge_fc2(merged_output)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "# Attention-based model\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_size, num_heads=16)\n",
    "\n",
    "        # Simple feedforward layer\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Apply attention mechanism\n",
    "        attention_output, _ = self.attention(input_data, input_data, input_data)\n",
    "\n",
    "        # Take the output of the attention mechanism and pass it through a linear layer\n",
    "        output = self.fc(attention_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Feedforward model\n",
    "class FeedforwardModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FeedforwardModel, self).__init__()\n",
    "\n",
    "        # Simplified feedforward layers\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = self.fc1(input_data)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "print(\"Normal\")\n",
    "# List to store models for each base station\n",
    "base_station_models = []\n",
    "\n",
    "# Training each base station model\n",
    "for base_station in range(num_base_stations):\n",
    "    model = HybridBeamformingModel(input_size=64, output_size=512)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(input_segments_train[base_station], output_segments_train[base_station])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    num_epochs = 20\n",
    "    total_batches = len(train_dataloader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training Base Station {base_station + 1}, Epoch {epoch + 1}\")\n",
    "        epoch_losses = []\n",
    "        for batch_idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_inputs)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "            # Print progress\n",
    "            percent_complete = (batch_idx + 1) / total_batches * 100\n",
    "            sys.stdout.write(f\"\\rProgress: [{int(percent_complete)}%]\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print()  # Move to the next line after completing an epoch\n",
    "        training_history[base_station].append(np.mean(epoch_losses))\n",
    "\n",
    "    base_station_models.append(model)\n",
    "\n",
    "# Evaluation on the test set\n",
    "input_segments_test = torch.chunk(torch.tensor(In_test), num_base_stations, dim=1)\n",
    "output_segments_test = torch.chunk(torch.tensor(Out_test), num_base_stations, dim=1)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            predictions = model(batch_inputs)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "base_station_losses = []\n",
    "# Evaluate each base station model\n",
    "for base_station, model in enumerate(base_station_models):\n",
    "    test_dataset = TensorDataset(input_segments_test[base_station], output_segments_test[base_station])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    test_loss = evaluate_model(model, test_dataloader)\n",
    "    base_station_losses.append(test_loss)\n",
    "    print(f\"Base Station {base_station + 1} Test Loss: {test_loss}\")\n",
    "\n",
    "# Calculate the mean of MSE for all base stations\n",
    "mean_mse = np.mean(base_station_losses)\n",
    "print(f\"Mean MSE across all Base Stations: {mean_mse}\")\n",
    "\n",
    "# Save the mean MSE to a CSV file\n",
    "mean_mse_df = pd.DataFrame({\"Mean_MSE\": [mean_mse]})\n",
    "mean_mse_df.to_csv(\"mean_mse_results.csv\", index=False)\n",
    "\n",
    "# Save the MSE values in the second sheet of mean_mse_results.csv\n",
    "mean_mse_df = pd.read_csv(\"mean_mse_results.csv\", index_col=None)\n",
    "\n",
    "# Create a DataFrame for all training history\n",
    "training_history_df = pd.DataFrame({\"Epoch\": range(1, num_epochs + 1)})\n",
    "\n",
    "# Append training history to the DataFrame for each base station\n",
    "for base_station, history in enumerate(training_history):\n",
    "    sheet_name = f\"Base_Station_{base_station + 1}_Training_History\"\n",
    "    training_history_df[sheet_name] = history\n",
    "\n",
    "    # Optionally, you can append the sheet name to the DataFrame for reference\n",
    "    mean_mse_df[sheet_name] = f\"base_station_{base_station + 1}.csv\"\n",
    "\n",
    "# Save the updated DataFrame to mean_mse_results.csv\n",
    "#mean_mse_df.to_csv(\"mean_mse_results.csv\", index=False)\n",
    "\n",
    "# Save the combined training history DataFrame to a single CSV file\n",
    "training_history_df.to_csv(\"combined_training_history.csv\", index=False)\n",
    "\n",
    "# Aggregate predictions from all base station models\n",
    "def predict(input_data):\n",
    "    input_segments = torch.chunk(torch.tensor(input_data), num_base_stations, dim=1)\n",
    "    predictions = []\n",
    "    for base_station, model in enumerate(base_station_models):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(input_segments[base_station])\n",
    "        predictions.append(output)\n",
    "    return torch.cat(predictions, dim=1)\n",
    "\n",
    "# Example usage\n",
    "sample_input = np.random.rand(1, 256).astype(np.float32)  # Convert to float32 if not already\n",
    "sample_input_tensor = torch.tensor(sample_input, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "predicted_output = predict(sample_input_tensor)\n",
    "\n",
    "# Now 'predicted_output' contains the aggregated predictions from all base station models.\n",
    "# You can further analyze or use these predictions as needed.\n",
    "# Plotting the training history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for base_station, history in enumerate(training_history):\n",
    "    plt.plot(range(1, num_epochs + 1), history, label=f\"Base Station {base_station + 1}\")\n",
    "\n",
    "plt.title('Training History of Beamforming Prediction Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set both the lower and upper y-axis limits\n",
    "plt.ylim(0.0005, plt.ylim()[1])\n",
    "\n",
    "# Save the figure as a PDF\n",
    "plt.savefig(\"training_history_plot.pdf\", bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "############ Attack the RF Beamforming Codeword Prediction Model ##########\n",
    "print(\"Attack\")\n",
    "def fgsm_attack(model, input_data, target_data, epsilon):\n",
    "    model.eval()\n",
    "    input_data.requires_grad = True\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = nn.MSELoss()(output, target_data)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM attack\n",
    "    input_data_grad = input_data.grad.data\n",
    "    perturbed_input = input_data + epsilon * torch.sign(input_data_grad)\n",
    "\n",
    "    return perturbed_input\n",
    "\n",
    "# Choose an epsilon value for the attack\n",
    "epsilon = 0.05\n",
    "\n",
    "# Generate adversarial examples for the test set\n",
    "adversarial_examples = []\n",
    "for base_station, model in enumerate(base_station_models):\n",
    "    input_data = input_segments_test[base_station].clone().detach().requires_grad_(True)\n",
    "    target_data = output_segments_test[base_station].clone().detach()\n",
    "\n",
    "    perturbed_input = fgsm_attack(model, input_data, target_data, epsilon)\n",
    "    adversarial_examples.append(perturbed_input)\n",
    "\n",
    "# Evaluate the model on the adversarial examples\n",
    "adversarial_losses = []\n",
    "for base_station, model in enumerate(base_station_models):\n",
    "    adversarial_dataset = TensorDataset(adversarial_examples[base_station], output_segments_test[base_station])\n",
    "    adversarial_dataloader = DataLoader(adversarial_dataset, batch_size=32, shuffle=False)\n",
    "    adversarial_loss = evaluate_model(model, adversarial_dataloader)\n",
    "    adversarial_losses.append(adversarial_loss)\n",
    "    print(f\"Base Station {base_station + 1} Adversarial Test Loss: {adversarial_loss}\")\n",
    "\n",
    "# Calculate the mean of MSE for all base stations on adversarial examples\n",
    "mean_adversarial_mse = np.mean(adversarial_losses)\n",
    "print(f\"Mean Adversarial MSE across all Base Stations: {mean_adversarial_mse}\")\n",
    "\n",
    "# Save the mean Adversarial MSE to a CSV file\n",
    "mean_adversarial_mse_df = pd.DataFrame({\"Mean_Adversarial_MSE\": [mean_adversarial_mse]})\n",
    "mean_adversarial_mse_df.to_csv(\"mean_adversarial_mse_results.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## Adversarial Training ######################\n",
    "print(\"Adversarial Training\")\n",
    "# Define a new HybridBeamformingModel for adversarial training\n",
    "adversarial_model = HybridBeamformingModel(input_size=64, output_size=512)\n",
    "adversarial_optimizer = torch.optim.Adam(adversarial_model.parameters(), lr=1e-3)\n",
    "adversarial_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Adversarial training loop with combined inputs\n",
    "num_adversarial_epochs = 10\n",
    "total_batches = len(train_dataloader)\n",
    "\n",
    "for adversarial_epoch in range(num_adversarial_epochs):\n",
    "    print(f\"Adversarial Training Epoch {adversarial_epoch + 1}\")\n",
    "    adversarial_losses = []\n",
    "\n",
    "    for batch_idx, (batch_inputs, batch_targets) in enumerate(train_dataloader):\n",
    "        # Regular training step\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_inputs)\n",
    "        loss = criterion(predictions, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adversarial training step\n",
    "        adversarial_inputs = fgsm_attack(adversarial_model, batch_inputs, batch_targets, epsilon)\n",
    "        adversarial_predictions = adversarial_model(adversarial_inputs)\n",
    "        adversarial_loss = adversarial_criterion(adversarial_predictions, batch_targets)\n",
    "        adversarial_loss.backward()\n",
    "        adversarial_optimizer.step()\n",
    "\n",
    "        adversarial_losses.append(adversarial_loss.item())\n",
    "\n",
    "        # Combine regular and adversarial inputs\n",
    "        combined_inputs = torch.cat([batch_inputs, adversarial_inputs], dim=0)\n",
    "        combined_targets = torch.cat([batch_targets, batch_targets], dim=0)\n",
    "\n",
    "        # Training step with combined inputs\n",
    "        optimizer.zero_grad()\n",
    "        combined_predictions = model(combined_inputs)\n",
    "        combined_loss = criterion(combined_predictions, combined_targets)\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        percent_complete = (batch_idx + 1) / total_batches * 100\n",
    "        sys.stdout.write(f\"\\rProgress: [{int(percent_complete)}%]\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print()  # Move to the next line after completing an epoch\n",
    "    training_history[base_station].append(np.mean(adversarial_losses))\n",
    "\n",
    "\n",
    "# Evaluate the adversarially trained model with combined inputs on the regular test set\n",
    "adversarial_model.eval()\n",
    "\n",
    "combined_test_losses = []\n",
    "for base_station, model in enumerate(base_station_models):\n",
    "    combined_test_dataset = TensorDataset(input_segments_test[base_station], output_segments_test[base_station])\n",
    "    combined_test_dataloader = DataLoader(combined_test_dataset, batch_size=32, shuffle=False)\n",
    "    combined_test_loss = evaluate_model(model, combined_test_dataloader)\n",
    "    combined_test_losses.append(combined_test_loss)\n",
    "    print(f\"Base Station {base_station + 1} Combined Test Loss: {combined_test_loss}\")\n",
    "\n",
    "# Calculate the mean of MSE for all base stations on the regular test set after adversarial training with combined inputs\n",
    "mean_combined_test_mse = np.mean(combined_test_losses)\n",
    "print(f\"Mean Combined Test MSE across all Base Stations: {mean_combined_test_mse}\")\n",
    "\n",
    "# Save the mean Combined Test MSE to a CSV file\n",
    "mean_combined_test_mse_df = pd.DataFrame({\"Mean_Combined_Test_MSE\": [mean_combined_test_mse]})\n",
    "mean_combined_test_mse_df.to_csv(\"mean_combined_test_mse_results.csv\", index=False)\n",
    "\n",
    "# Adversarial Training History Plot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b094e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
